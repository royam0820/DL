{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-transformers - GPT2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royam0820/DL/blob/master/pytorch_transformers_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdIxWUxisfi1",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Tranformers\n",
        "Ref.:  https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/\n",
        "\n",
        "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).\n",
        "\n",
        "This library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:\n",
        "\n",
        "- BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "- GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training\n",
        "- GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners\n",
        "- Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n",
        "- XLNet (from Google/CMU) released with the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
        "- XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining\n",
        "All of the above models are the best in class for various NLP tasks. Some of these models are as recent as the previous month!\n",
        "\n",
        "Most of the State-of-the-Art models require tons of training data and days of training on expensive GPU hardware which is something only the big technology companies and research labs can afford. But with the launch of PyTorch-Transformers, now anyone can utilize the power of State-of-the-Art models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alvczH58uRw8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMcz5-Xon-vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "02163098-2f1a-41d8-fb2a-bb584af6713a"
      },
      "source": [
        "# installing the pytorch-transformers\n",
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 81kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.189)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
            "Collecting sentencepiece (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Collecting regex (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.189)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: sentencepiece, regex, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.0.0 regex-2019.6.8 sentencepiece-0.1.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJYjgdMDrGuq",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the next word in GPT2\n",
        "The code below is straightforward. We tokenize and index the text as a sequence of numbers and pass it to the GPT2LMHeadModel. This is nothing but the GPT2 model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-763hxof7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgAh6KSvokEZ",
        "colab_type": "code",
        "outputId": "be79616f-57fa-4c63-ef30-f2de5863bf38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:01<00:00, 946275.44B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 616929.18B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "carWz7b0opce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode a text inputs (English)\n",
        "text = \"What is the fastest car in the\"\n",
        "indexed_tokens = tokenizer.encode(text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kP8HkEZowJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8-rqYF0o2CX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypg6qPuWo6Ew",
        "colab_type": "code",
        "outputId": "2fc3fa59-7f6e-4421-e8e0-8abbc291a3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb8A-UW1o9sI",
        "colab_type": "code",
        "outputId": "89fd71af-0dc0-4465-9f44-fae9fb59f21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): BertLayerNorm()\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1)\n",
              "          (resid_dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (ln_2): BertLayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): BertLayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYEQmBy5o-yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a__LTOqpEkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZgVljaSpHHG",
        "colab_type": "code",
        "outputId": "aec776ad-6353-4c2a-cfe3-20617b31b5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Print the predicted word\n",
        "print(predicted_text)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the fastest car in the world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIQe7diDt7f_",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  Awesome! The model successfully predicts the next word as “world”. This is pretty amazing as this is what Google was suggesting. I recommend you try this model with different input sentences and see how it performs while predicting the next word in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCKhZk2EpxCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbQ1A9OetoTi",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Generation using GPT-2, Transformer-XL and XLNet\n",
        "\n",
        "Let’s take Text Generation to the next level now. Instead of predicting only the next word, we will generate a paragraph of text based on the given input. Let’s see what output our models give for the following input text:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
        "\n",
        "```\n",
        "\n",
        "We will be using the readymade script that PyTorch-Transformers provides for this task. Let’s clone their repository first:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qRhfbzuOSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "16e1039d-b2c3-443e-e43d-f0047f25a809"
      },
      "source": [
        "!git clone https://github.com/huggingface/pytorch-transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-transformers'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/48)   \u001b[K\rremote: Counting objects:   4% (2/48)   \u001b[K\rremote: Counting objects:   6% (3/48)   \u001b[K\rremote: Counting objects:   8% (4/48)   \u001b[K\rremote: Counting objects:  10% (5/48)   \u001b[K\rremote: Counting objects:  12% (6/48)   \u001b[K\rremote: Counting objects:  14% (7/48)   \u001b[K\rremote: Counting objects:  16% (8/48)   \u001b[K\rremote: Counting objects:  18% (9/48)   \u001b[K\rremote: Counting objects:  20% (10/48)   \u001b[K\rremote: Counting objects:  22% (11/48)   \u001b[K\rremote: Counting objects:  25% (12/48)   \u001b[K\rremote: Counting objects:  27% (13/48)   \u001b[K\rremote: Counting objects:  29% (14/48)   \u001b[K\rremote: Counting objects:  31% (15/48)   \u001b[K\rremote: Counting objects:  33% (16/48)   \u001b[K\rremote: Counting objects:  35% (17/48)   \u001b[K\rremote: Counting objects:  37% (18/48)   \u001b[K\rremote: Counting objects:  39% (19/48)   \u001b[K\rremote: Counting objects:  41% (20/48)   \u001b[K\rremote: Counting objects:  43% (21/48)   \u001b[K\rremote: Counting objects:  45% (22/48)   \u001b[K\rremote: Counting objects:  47% (23/48)   \u001b[K\rremote: Counting objects:  50% (24/48)   \u001b[K\rremote: Counting objects:  52% (25/48)   \u001b[K\rremote: Counting objects:  54% (26/48)   \u001b[K\rremote: Counting objects:  56% (27/48)   \u001b[K\rremote: Counting objects:  58% (28/48)   \u001b[K\rremote: Counting objects:  60% (29/48)   \u001b[K\rremote: Counting objects:  62% (30/48)   \u001b[K\rremote: Counting objects:  64% (31/48)   \u001b[K\rremote: Counting objects:  66% (32/48)   \u001b[K\rremote: Counting objects:  68% (33/48)   \u001b[K\rremote: Counting objects:  70% (34/48)   \u001b[K\rremote: Counting objects:  72% (35/48)   \u001b[K\rremote: Counting objects:  75% (36/48)   \u001b[K\rremote: Counting objects:  77% (37/48)   \u001b[K\rremote: Counting objects:  79% (38/48)   \u001b[K\rremote: Counting objects:  81% (39/48)   \u001b[K\rremote: Counting objects:  83% (40/48)   \u001b[K\rremote: Counting objects:  85% (41/48)   \u001b[K\rremote: Counting objects:  87% (42/48)   \u001b[K\rremote: Counting objects:  89% (43/48)   \u001b[K\rremote: Counting objects:  91% (44/48)   \u001b[K\rremote: Counting objects:  93% (45/48)   \u001b[K\rremote: Counting objects:  95% (46/48)   \u001b[K\rremote: Counting objects:  97% (47/48)   \u001b[K\rremote: Counting objects: 100% (48/48)   \u001b[K\rremote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 5594 (delta 21), reused 40 (delta 15), pack-reused 5546\u001b[K\n",
            "Receiving objects: 100% (5594/5594), 3.07 MiB | 5.22 MiB/s, done.\n",
            "Resolving deltas: 100% (3968/3968), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVsM-CnFvBFY",
        "colab_type": "text"
      },
      "source": [
        "## GPT2\n",
        "Now, you just need a single command to start the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6nMIeaCvLQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "6e57a91d-f2f7-451d-e10a-82ce479d3da6"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=gpt2 \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/27/2019 02:47:19 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "07/27/2019 02:47:19 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "07/27/2019 02:47:20 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "07/27/2019 02:47:20 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"token_ids\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "07/27/2019 02:47:21 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "Namespace(device=device(type='cuda'), length=100, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, padding_text='', prompt='', seed=42, temperature=1.0, top_k=0, top_p=0.9)\n",
            "Model prompt >>> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "100% 100/100 [00:03<00:00, 24.00it/s]\n",
            " The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6.\n",
            "\n",
            "What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a\n",
            "Model prompt >>> "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuaIQLyXv-l6",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  the result: \n",
        "\n",
        "\n",
        "```\n",
        "The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6. What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Awsome! The text that the model generated is very cohesive and actually can be mistaken as a real news article.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vYTb47wTVU",
        "colab_type": "text"
      },
      "source": [
        "## XL-NET\n",
        "XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin. XLNet achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.\n",
        "\n",
        "You can use this text to test it:\n",
        "\n",
        "```\n",
        "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
        "```\n",
        "\n",
        "You can use the following code for the same:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNIswSfxwEy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "637f537c-f12c-46e6-8853-3485c3bf1c77"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\n",
        "    --model_type=xlnet \\\n",
        "    --length=50 \\\n",
        "    --model_name_or_path=xlnet-base-cased \\"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/27/2019 03:14:58 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model not found in cache, downloading to /tmp/tmptdmaw3k8\n",
            "100% 798011/798011 [00:00<00:00, 1935899.61B/s]\n",
            "07/27/2019 03:14:59 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmptdmaw3k8 to cache at /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "07/27/2019 03:14:59 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "07/27/2019 03:14:59 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmptdmaw3k8\n",
            "07/27/2019 03:14:59 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "07/27/2019 03:14:59 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json not found in cache, downloading to /tmp/tmp8ehom421\n",
            "100% 641/641 [00:00<00:00, 357282.24B/s]\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp8ehom421 to cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp8ehom421\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_token\": 32000,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "07/27/2019 03:15:00 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmpt32ke5_s\n",
            "100% 467042463/467042463 [00:16<00:00, 28425859.40B/s]\n",
            "07/27/2019 03:15:17 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpt32ke5_s to cache at /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "07/27/2019 03:15:18 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "07/27/2019 03:15:18 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpt32ke5_s\n",
            "07/27/2019 03:15:18 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "Namespace(device=device(type='cuda'), length=50, model_name_or_path='xlnet-base-cased', model_type='xlnet', n_gpu=1, no_cuda=False, padding_text='', prompt='', seed=42, temperature=1.0, top_k=0, top_p=0.9)\n",
            "Model prompt >>> The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6. What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a\n",
            "100% 50/50 [00:06<00:00,  7.97it/s]\n",
            "bit sooner and because we could use this research data to develop a problem for humans to respond to.<eop> Important, More Important, Unimportant Good. The peak of the science for this blog at this time was just around this time. Human contacts in\n",
            "Model prompt >>> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "100% 50/50 [00:04<00:00,  9.27it/s]\n",
            "Even more surprising to the scientists was the fact that \"U\" was \"He\" in an actual human language. The \"U\" as \"He\" was translated into a Japanese language, which was a very difficult process for the\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"pytorch-transformers/examples/run_generation.py\", line 195, in <module>\n",
            "    main()\n",
            "  File \"pytorch-transformers/examples/run_generation.py\", line 171, in main\n",
            "    raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByLL9ATJxpss",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  here is the result:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Even more surprising to the scientists was the fact that \"U\" was \"He\" in an actual human language. The \"U\" as \"He\" was translated into a Japanese language, which was a very difficult process for the\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Interesting. While the GPT-2 model focussed directly on the scientific angle of the news about unicorns, XLNet actually nicely built up the context and subtly introduced the topic of unicorns. Let’s see how does Transformer-XL performs!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfC2O0qbyGiT",
        "colab_type": "text"
      },
      "source": [
        "## Transformer XL\n",
        "Transformer networks are limited by a fixed-length context and thus can be improved through learning longer-term dependency. That’s why Google proposed a novel method called Transformer-XL (meaning extra long) for language modeling, which enables a Transformer architecture to learn longer-term dependency.\n",
        "\n",
        "**Transformer-XL is up to 1800 times faster than a typical Transformer. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aKb7L_VyNQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5df74f8c-7fd7-4313-ca26-99a1a02e4606"
      },
      "source": [
        "!python pytorch-transformers/examples/run_generation.py \\\n",
        "    --model_type=transfo-xl \\\n",
        "    --length=100 \\\n",
        "    --model_name_or_path=transfo-xl-wt103 \\"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/27/2019 03:22:55 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin not found in cache, downloading to /tmp/tmpoxc523na\n",
            "100% 9143613/9143613 [00:00<00:00, 10863199.69B/s]\n",
            "07/27/2019 03:22:56 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpoxc523na to cache at /root/.cache/torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
            "07/27/2019 03:22:56 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
            "07/27/2019 03:22:56 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpoxc523na\n",
            "07/27/2019 03:22:56 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin from cache at /root/.cache/torch/pytorch_transformers/b24cb708726fd43cbf1a382da9ed3908263e4fb8a156f9e0a4f45b7540c69caa.a6a9c41b856e5c31c9f125dd6a7ed4b833fbcefda148b627871d4171b25cffd1\n",
            "07/27/2019 03:22:57 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json not found in cache, downloading to /tmp/tmpz4ph7xuo\n",
            "100% 606/606 [00:00<00:00, 323542.29B/s]\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpz4ph7xuo to cache at /root/.cache/torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpz4ph7xuo\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json from cache at /root/.cache/torch/pytorch_transformers/a6dfd6a3896b3ae4c1a3c5f26ff1f1827c26c15b679de9212a04060eaf1237df.aef76fb1064c932cd6a2a2be3f23ebbfa5f9b6e29e8e87b571c45b4a5d5d1b90\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"adaptive\": true,\n",
            "  \"attn_type\": 0,\n",
            "  \"clamp_len\": 1000,\n",
            "  \"cutoffs\": [\n",
            "    20000,\n",
            "    40000,\n",
            "    200000\n",
            "  ],\n",
            "  \"d_embed\": 1024,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 4096,\n",
            "  \"d_model\": 1024,\n",
            "  \"div_val\": 4,\n",
            "  \"dropatt\": 0.0,\n",
            "  \"dropout\": 0.1,\n",
            "  \"ext_len\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"init\": \"normal\",\n",
            "  \"init_range\": 0.01,\n",
            "  \"init_std\": 0.02,\n",
            "  \"mem_len\": 1600,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 18,\n",
            "  \"n_token\": 267735,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pre_lnorm\": false,\n",
            "  \"proj_init_std\": 0.01,\n",
            "  \"same_length\": true,\n",
            "  \"sample_softmax\": -1,\n",
            "  \"tgt_len\": 128,\n",
            "  \"tie_projs\": [\n",
            "    false,\n",
            "    true,\n",
            "    true,\n",
            "    true\n",
            "  ],\n",
            "  \"tie_weight\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "07/27/2019 03:22:58 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin not found in cache, downloading to /tmp/tmplt8p3i8c\n",
            "100% 1140884800/1140884800 [00:35<00:00, 31711200.91B/s]\n",
            "07/27/2019 03:23:34 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmplt8p3i8c to cache at /root/.cache/torch/pytorch_transformers/12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n",
            "07/27/2019 03:23:39 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n",
            "07/27/2019 03:23:39 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmplt8p3i8c\n",
            "07/27/2019 03:23:39 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/12642ff7d0279757d8356bfd86a729d9697018a0c93ad042de1d0d2cc17fd57b.e9704971f27275ec067a00a67e6a5f0b05b4306b3f714a96e9f763d8fb612671\n",
            "Namespace(device=device(type='cuda'), length=100, model_name_or_path='transfo-xl-wt103', model_type='transfo-xl', n_gpu=1, no_cuda=False, padding_text='', prompt='', seed=42, temperature=1.0, top_k=0, top_p=0.9)\n",
            "Model prompt >>> In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "100% 100/100 [00:43<00:00,  2.12it/s]\n",
            "language ; both never spoke in their native language ( a natural language ). If they are speaking in their native language they will have no communication with the original speakers. The encounter with a dingo brought between two and four unicorns to a head at once, thus crossing the border into Peru to avoid internecine warfare, as they did with the Aztecs. On September 11, 1930, three armed robbers killed a donkey for helping their fellow soldiers fight alongside a group of Argentines. During the same year, a pygmy @-@\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"pytorch-transformers/examples/run_generation.py\", line 195, in <module>\n",
            "    main()\n",
            "  File \"pytorch-transformers/examples/run_generation.py\", line 171, in main\n",
            "    raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbKPtyXHzDiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRH051HEzWsi",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  here is the result:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "language ; both never spoke in their native language ( a natural language ). If they are speaking in their native language they will have no communication with the original speakers. The encounter with a dingo brought between two and four unicorns to a head at once, thus crossing the border into Peru to avoid internecine warfare, as they did with the Aztecs. On September 11, 1930, three armed robbers killed a donkey for helping their fellow soldiers fight alongside a group of Argentines. During the same year, a pygmy @-@\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Now, this is awesome. It is interesting to see how different models focus on different aspects of the input text to generate further. This variation is due to a lot of factors but mostly can be attributed to different training data and model architectures.\n",
        "\n",
        "But there’s a caveat. Neural text generation has been facing a bit of backlash in recent times as people worry it can increase problems related to fake news. But think about the positive side of it! We can use it for many positive applications like- helping writers/creatives with new ideas, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXfj10htzYzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6FTWLF1eFUG",
        "colab_type": "text"
      },
      "source": [
        "# Training a Masked Language Model for BERT\n",
        "\n",
        "The BERT framework, a new language representation model from Google AI, uses pre-training and fine-tuning to create state-of-the-art NLP models for a wide range of tasks. These tasks include question answering systems, sentiment analysis, and language inference.\n",
        "\n",
        "BERT is pre-trained using the following two unsupervised prediction tasks:\n",
        "\n",
        "Masked Language Modeling (MLM)\n",
        "- Next Sentence Prediction\n",
        "- And you can implement both of these using PyTorch-Transformers. In fact, you can build your own BERT model from scratch or fine-tune a pre-trained version. So, let’s see how can we implement the Masked Language Model for BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU11B8SJecRp",
        "colab_type": "text"
      },
      "source": [
        "## Problem Definition\n",
        "Let’s formally define our problem statement:\n",
        "\n",
        "```\n",
        "Given an input sequence, we will randomly mask some words. The model then should predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n",
        "```\n",
        "\n",
        "First, let’s prepare a tokenized input from a text string using `BertTokenizer:``\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ajeb-bbeHFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "583c0b1b-1ac6-4ced-9fb9-b53c92b008c3"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 935757.46B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz2BovkAe34W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSD5V4KwfJS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "02b811fe-3f6d-4fcd-bd32-f941f87e09ef"
      },
      "source": [
        "tokenized_text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'who',\n",
              " 'was',\n",
              " 'jim',\n",
              " 'henson',\n",
              " '?',\n",
              " '[SEP]',\n",
              " 'jim',\n",
              " 'henson',\n",
              " 'was',\n",
              " 'a',\n",
              " 'puppet',\n",
              " '##eer',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5zyFHCqfVDb",
        "colab_type": "text"
      },
      "source": [
        "## Next Step\n",
        "The next step would be to convert this into a sequence of integers and create PyTorch tensors of them so that we can use them directly for computation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlYS0RrQfMSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "masked_index = 8\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
        "\n",
        "# Convert token to vocabulary indices\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scN79pnMfsh4",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  **Notice that we have set [MASK] at the 8th index in the sentence which is the word ‘Hensen’. This is what our model will try to predict.**\n",
        "\n",
        "Now that our data is rightly pre-processed for BERT, we will create a Masked Language Model. Let’s now use `BertForMaskedLM` to predict a masked token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d0zLoamfjRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "74b2b3ff-a88e-4453-fad7-2ea7ac5d6f4c"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "segments_tensors = segments_tensors.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "assert predicted_token == 'henson'\n",
        "print('Predicted token is:',predicted_token)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 74224.98B/s]\n",
            "100%|██████████| 440473133/440473133 [00:16<00:00, 26547031.57B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted token is: henson\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBbr27frgbiS",
        "colab_type": "text"
      },
      "source": [
        "NOTE:  That’s quite impressive.\n",
        "\n",
        "This was a small demo of training a Masked Language Model on a single input sequence. Nevertheless, it is a very important part of the training process for many Transformer-based architectures. This is because it allows bidirectional training in models – which was previously impossible.\n",
        "\n",
        "Congratulations! You’ve just implemented your first Masked Language Model! If you were trying to train BERT, you just finished half your work. This example will have given you a good idea of how to use PyTorch-Transformers to work with the BERT model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh6TkgYhgrIX",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "We hav  implemented and explored various State-of-the-Art NLP models like BERT, GPT-2, Transformer-XL, and XLNet using PyTorch-Transformers. This was more like a firest impressions expertiment that I did to give you a good intuition on how to work with this amazing library.\n",
        "\n",
        "Here are 6 compelling reasons why I think you would love this library:\n",
        "\n",
        "- **Pre-trained models**: It provides pre-trained models for 6 State-of-the-Art NLP architectures and pre-trained weights for 27 variations of these models\n",
        "- **Preprocessing and Finetuning API**: PyTorch-Transformers doesn’t stop at pre-trained weights. It also provides a simple API for doing all the preprocessing and finetuning steps required for these models. Now, if you have read recent research papers, you’d know many of the State-of-the-Art models have unique ways of preprocessing the data and a lot of times it becomes a hassle to write code for the entire preprocessing pipeline\n",
        "- **Usage scripts**: It also comes with scripts to run these models against benchmark NLP datasets like SQUAD 2.0 (Stanford Question Answering Dataset), and GLUE (General Language Understanding Evaluation). By using - - -PyTorch-Transformers, you can directly run your model against these datasets and evaluate the performance accordingly\n",
        "- **Multilingual**: PyTorch-Transformers has multilingual support. This is because some of the models already work well for multiple languages\n",
        "- TensorFlow Compatibility: You can import TensorFlow checkpoints as models in PyTorch\n",
        "- **BERTology**: There is a growing field of study concerned with investigating the inner working of large-scale transformers like BERT (that some call “BERTology”)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUzzSZbHgtGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}